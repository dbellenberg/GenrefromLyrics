{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classification(train, label, vectorizer='bow', n_top_features=10, oversample=False):\n",
    "\n",
    "    train = train.apply(' '.join)\n",
    "\n",
    "    # split into train and test sets, with stratifying\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(train, label, test_size=0.3, random_state=42, stratify=label)\n",
    "\n",
    "    # Split the temporary test set into 50% test and 50% validation (15% of the total data each)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    if vectorizer == 'bow':\n",
    "        vec = CountVectorizer()\n",
    "    elif vectorizer == 'tfidf':\n",
    "        vec = TfidfVectorizer()\n",
    "\n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "\n",
    "    # Initialize the MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # Perform random oversampling if enabled\n",
    "    if oversample:\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "        X_train_vec, y_train = oversampler.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    nb.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Print the most informative features\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    for i, class_label in enumerate(nb.classes_):\n",
    "        print(f\"\\nClass: {class_label}\")\n",
    "        top_features_idx = nb.feature_log_prob_[i].argsort()[-n_top_features:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_features_idx]\n",
    "        print(\", \".join(top_features))\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = nb.predict(X_test_vec)\n",
    "    y_val_pred = nb.predict(X_val_vec)\n",
    "\n",
    "    # Generate classification report\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "    return test_report, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_classification(train, label, vectorizer_method='tfidf', oversample=False, n_top_features=10):\n",
    "\n",
    "    train = train.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # split into train and test sets, with stratifying\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(train, label, test_size=0.3, random_state=42, stratify=label)\n",
    "\n",
    "    # Split the temporary test set into 50% test and 50% validation (15% of the total data each)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    # Initialize vectorizer (TDIDF OR bag of words)\n",
    "    if vectorizer_method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif vectorizer_method == 'bow':\n",
    "        vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Vectorize data\n",
    "    X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "    X_val_vectors = vectorizer.transform(X_val)\n",
    "    X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "    # Perform oversampling\n",
    "    if oversample == True:\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "        X_train_vectors, y_train = oversampler.fit_resample(X_train_vectors, y_train)\n",
    "\n",
    "    # Initialize the Multinomial LR\n",
    "    lr_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "    # Fit the model to the training data \n",
    "    lr_clf.fit(X_train_vectors, y_train)\n",
    "  \n",
    "    # Get the feature names from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "  \n",
    "    # Print the most informative features\n",
    "    for i, label in enumerate(lr_clf.classes_):\n",
    "        print(f\"\\nClass: {label}\")\n",
    "        top_features_idx = lr_clf.coef_[i].argsort()[-n_top_features:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_features_idx]\n",
    "        print(\", \".join(top_features))\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = lr_clf.predict(X_test_vectors)\n",
    "    y_val_pred = lr_clf.predict(X_val_vectors)\n",
    "\n",
    "    # Generate classification report\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "    return test_report, val_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./pkl_files/lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>lyrics_word_count</th>\n",
       "      <th>lemmatized_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>[killa, cam, killa, cam, cam, killa, cam, kill...</td>\n",
       "      <td>1</td>\n",
       "      <td>762</td>\n",
       "      <td>[killa, cam, killa, cam, cam, killa, cam, kill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>[yeah, hah, yeah, rocafella, invite, somethin,...</td>\n",
       "      <td>3</td>\n",
       "      <td>548</td>\n",
       "      <td>[yeah, hah, yeah, rocafella, invite, somethin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>[maybe, cause, im, eatin, bastards, fiend, gru...</td>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "      <td>[maybe, cause, im, eatin, bastard, fiend, grub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>[ugh, killa, baby, kanye, 1970s, heron, flow, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>760</td>\n",
       "      <td>[ugh, killa, baby, kanye, 1970s, heron, flow, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>[ask, young, boy, gon, second, time, around, g...</td>\n",
       "      <td>6</td>\n",
       "      <td>432</td>\n",
       "      <td>[ask, young, boy, gon, second, time, around, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   \n",
       "0          Killa Cam  rap    Cam'ron  2004  \\\n",
       "1         Can I Live  rap      JAY-Z  1996   \n",
       "2  Forgive Me Father  rap   Fabolous  2003   \n",
       "3       Down and Out  rap    Cam'ron  2004   \n",
       "4             Fly In  rap  Lil Wayne  2005   \n",
       "\n",
       "                                              lyrics  id  lyrics_word_count   \n",
       "0  [killa, cam, killa, cam, cam, killa, cam, kill...   1                762  \\\n",
       "1  [yeah, hah, yeah, rocafella, invite, somethin,...   3                548   \n",
       "2  [maybe, cause, im, eatin, bastards, fiend, gru...   4                574   \n",
       "3  [ugh, killa, baby, kanye, 1970s, heron, flow, ...   5                760   \n",
       "4  [ask, young, boy, gon, second, time, around, g...   6                432   \n",
       "\n",
       "                                   lemmatized_lyrics  \n",
       "0  [killa, cam, killa, cam, cam, killa, cam, kill...  \n",
       "1  [yeah, hah, yeah, rocafella, invite, somethin,...  \n",
       "2  [maybe, cause, im, eatin, bastard, fiend, grub...  \n",
       "3  [ugh, killa, baby, kanye, 1970s, heron, flow, ...  \n",
       "4  [ask, young, boy, gon, second, time, around, g...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "im, love, like, know, dont, got, na, time, go, one\n",
      "\n",
      "Class: pop\n",
      "im, know, love, dont, like, na, oh, got, go, time\n",
      "\n",
      "Class: rap\n",
      "im, like, got, know, nigga, get, yeah, dont, bitch, shit\n",
      "\n",
      "Class: rb\n",
      "know, love, im, yeah, dont, like, got, baby, na, oh\n",
      "\n",
      "Class: rock\n",
      "im, know, dont, like, time, love, one, go, na, never\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.33      0.17      0.22     12677\n",
      "         pop       0.65      0.49      0.56    215359\n",
      "         rap       0.83      0.79      0.81    149486\n",
      "          rb       0.25      0.34      0.29     23802\n",
      "        rock       0.39      0.62      0.48     95954\n",
      "\n",
      "    accuracy                           0.59    497278\n",
      "   macro avg       0.49      0.48      0.47    497278\n",
      "weighted avg       0.63      0.59      0.60    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.32      0.17      0.22     12678\n",
      "         pop       0.65      0.49      0.56    215358\n",
      "         rap       0.83      0.79      0.81    149487\n",
      "          rb       0.24      0.34      0.28     23802\n",
      "        rock       0.40      0.62      0.48     95953\n",
      "\n",
      "    accuracy                           0.59    497278\n",
      "   macro avg       0.49      0.48      0.47    497278\n",
      "weighted avg       0.63      0.59      0.60    497278\n",
      "\n",
      "CPU times: user 6min 20s, sys: 16.8 s, total: 6min 37s\n",
      "Wall time: 6min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes & Bag of Words\n",
    "lemmatized_nb_bow_test_report, lemmatized_nb_bow_val_report = nb_classification(df['lemmatized_lyrics'], df['tag'], vectorizer='bow')\n",
    "print(lemmatized_nb_bow_test_report, lemmatized_nb_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "love, im, know, dont, like, oh, time, got, one, na\n",
      "\n",
      "Class: pop\n",
      "love, im, know, dont, oh, like, na, youre, time, go\n",
      "\n",
      "Class: rap\n",
      "im, nigga, like, got, bitch, yeah, get, shit, dont, know\n",
      "\n",
      "Class: rb\n",
      "love, baby, yeah, know, im, dont, oh, na, got, like\n",
      "\n",
      "Class: rock\n",
      "im, know, dont, time, love, youre, never, one, oh, like\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.00      0.00      0.00     12677\n",
      "         pop       0.58      0.90      0.71    215359\n",
      "         rap       0.80      0.83      0.82    149486\n",
      "          rb       0.15      0.00      0.00     23802\n",
      "        rock       0.70      0.05      0.09     95954\n",
      "\n",
      "    accuracy                           0.65    497278\n",
      "   macro avg       0.45      0.36      0.32    497278\n",
      "weighted avg       0.63      0.65      0.57    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.00      0.00      0.00     12678\n",
      "         pop       0.58      0.91      0.71    215358\n",
      "         rap       0.80      0.83      0.82    149487\n",
      "          rb       0.12      0.00      0.00     23802\n",
      "        rock       0.69      0.05      0.09     95953\n",
      "\n",
      "    accuracy                           0.65    497278\n",
      "   macro avg       0.44      0.36      0.32    497278\n",
      "weighted avg       0.63      0.65      0.57    497278\n",
      "\n",
      "CPU times: user 6min 22s, sys: 15.8 s, total: 6min 38s\n",
      "Wall time: 6min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes & TFIDF\n",
    "lemmatized_nb_tfidf_test_report, lemmatized_nb_tfidf_val_report = nb_classification(df['lemmatized_lyrics'], df['tag'], vectorizer='tfidf')\n",
    "print(lemmatized_nb_tfidf_test_report, lemmatized_nb_tfidf_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "im, love, like, know, dont, got, na, time, go, one\n",
      "\n",
      "Class: pop\n",
      "im, know, love, dont, like, na, oh, got, go, time\n",
      "\n",
      "Class: rap\n",
      "im, like, got, know, nigga, get, yeah, dont, bitch, shit\n",
      "\n",
      "Class: rb\n",
      "know, love, im, yeah, dont, like, got, baby, na, oh\n",
      "\n",
      "Class: rock\n",
      "im, know, dont, like, time, love, one, go, never, oh\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.13      0.67      0.22     12677\n",
      "         pop       0.70      0.18      0.28    215359\n",
      "         rap       0.86      0.75      0.80    149486\n",
      "          rb       0.17      0.59      0.26     23802\n",
      "        rock       0.38      0.65      0.48     95954\n",
      "\n",
      "    accuracy                           0.47    497278\n",
      "   macro avg       0.45      0.57      0.41    497278\n",
      "weighted avg       0.65      0.47      0.47    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.13      0.68      0.22     12678\n",
      "         pop       0.70      0.18      0.28    215358\n",
      "         rap       0.86      0.75      0.80    149487\n",
      "          rb       0.17      0.59      0.26     23802\n",
      "        rock       0.38      0.65      0.48     95953\n",
      "\n",
      "    accuracy                           0.47    497278\n",
      "   macro avg       0.45      0.57      0.41    497278\n",
      "weighted avg       0.65      0.47      0.47    497278\n",
      "\n",
      "CPU times: user 6min 27s, sys: 17.4 s, total: 6min 44s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Oversampling with Naive Bayes & Bag of Words\n",
    "lemmatized_nb_ros_bow_test_report, lemmatized_nb_ros_bow_val_report = nb_classification(df['lemmatized_lyrics'], df['tag'], vectorizer='bow', oversample=True)\n",
    "print(lemmatized_nb_ros_bow_test_report, lemmatized_nb_ros_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "lonesome, porch, whiskey, guitar, heartache, truck, tennessee, texas, tailgate, hed\n",
      "\n",
      "Class: pop\n",
      "fcking, fk, rap, chuckle, fck, pre, fking, spoken, nigga, evry\n",
      "\n",
      "Class: rap\n",
      "snippet, rapping, rapper, lyric, rap, intro, fam, spitting, depression, opps\n",
      "\n",
      "Class: rb\n",
      "pre, outro, tryna, trynna, 2x, hook, nigga, imma, intro, shawty\n",
      "\n",
      "Class: rock\n",
      "thе, endless, fz, disease, guitar, punk, failure, decay, drag, collapse\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.47      0.11      0.18     12677\n",
      "         pop       0.61      0.87      0.71    215359\n",
      "         rap       0.87      0.82      0.85    149486\n",
      "          rb       0.41      0.08      0.14     23802\n",
      "        rock       0.58      0.24      0.34     95954\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.59      0.43      0.44    497278\n",
      "weighted avg       0.67      0.68      0.64    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.47      0.11      0.18     12678\n",
      "         pop       0.61      0.87      0.71    215358\n",
      "         rap       0.87      0.82      0.85    149487\n",
      "          rb       0.41      0.08      0.14     23802\n",
      "        rock       0.57      0.24      0.34     95953\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.58      0.42      0.44    497278\n",
      "weighted avg       0.67      0.68      0.64    497278\n",
      "\n",
      "CPU times: user 5min 47s, sys: 43.5 s, total: 6min 31s\n",
      "Wall time: 16min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic regression & Bag of Words\n",
    "lemmatized_lr_bow_test_report, lemmatized_lr_bow_val_report = lr_classification(df['lemmatized_lyrics'], df['tag'], vectorizer_method='bow', n_top_features=10)\n",
    "print(lemmatized_lr_bow_test_report, lemmatized_lr_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "whiskey, country, truck, beer, cowboy, old, town, id, heartache, lonesome\n",
      "\n",
      "Class: pop\n",
      "nigga, fcking, repeat, pre, fck, yo, co, dey, spoken, fk\n",
      "\n",
      "Class: rap\n",
      "rap, rapper, rhyme, nigga, li, hook, bro, gang, lil, bitch\n",
      "\n",
      "Class: rb\n",
      "tryna, hook, ima, nigga, imma, pre, vibe, shit, x2, shawty\n",
      "\n",
      "Class: rock\n",
      "thе, fucking, well, punk, guitar, sick, drag, goddamn, death, blood\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.51      0.13      0.21     12677\n",
      "         pop       0.62      0.85      0.72    215359\n",
      "         rap       0.86      0.85      0.85    149486\n",
      "          rb       0.46      0.09      0.15     23802\n",
      "        rock       0.57      0.27      0.37     95954\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.61      0.44      0.46    497278\n",
      "weighted avg       0.67      0.68      0.65    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.49      0.13      0.21     12678\n",
      "         pop       0.62      0.85      0.72    215358\n",
      "         rap       0.86      0.85      0.85    149487\n",
      "          rb       0.45      0.09      0.15     23802\n",
      "        rock       0.57      0.27      0.36     95953\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.60      0.44      0.46    497278\n",
      "weighted avg       0.67      0.68      0.65    497278\n",
      "\n",
      "CPU times: user 5min 47s, sys: 41.3 s, total: 6min 29s\n",
      "Wall time: 16min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic regression & TDIDF\n",
    "lemmatized_lr_tfidf_test_report, lemmatized_lr_tfidf_val_report = lr_classification(df['lemmatized_lyrics'], df['tag'], vectorizer_method='tfidf', n_top_features=10)\n",
    "print(lemmatized_lr_tfidf_test_report, lemmatized_lr_tfidf_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "truck, whiskey, beer, cowboy, bar, ol, porch, instrumental, country, tennessee\n",
      "\n",
      "Class: pop\n",
      "nigga, co, repeat, bitch, yo, spoken, club, pre, fcking, colour\n",
      "\n",
      "Class: rap\n",
      "rap, rapper, tryna, hook, rhyme, nigga, li, bro, imma, mic\n",
      "\n",
      "Class: rb\n",
      "tryna, hook, pre, nigga, imma, ima, vibe, yo, outro, yall\n",
      "\n",
      "Class: rock\n",
      "punk, disease, drag, teeth, void, guitar, fucking, destroy, failure, thе\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.15      0.70      0.24     12677\n",
      "         pop       0.69      0.30      0.42    215359\n",
      "         rap       0.89      0.79      0.83    149486\n",
      "          rb       0.18      0.60      0.28     23802\n",
      "        rock       0.43      0.60      0.50     95954\n",
      "\n",
      "    accuracy                           0.53    497278\n",
      "   macro avg       0.47      0.60      0.46    497278\n",
      "weighted avg       0.66      0.53      0.55    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.15      0.70      0.24     12678\n",
      "         pop       0.69      0.30      0.42    215358\n",
      "         rap       0.89      0.79      0.83    149487\n",
      "          rb       0.18      0.59      0.28     23802\n",
      "        rock       0.43      0.60      0.50     95953\n",
      "\n",
      "    accuracy                           0.53    497278\n",
      "   macro avg       0.47      0.60      0.46    497278\n",
      "weighted avg       0.66      0.53      0.55    497278\n",
      "\n",
      "CPU times: user 6min 1s, sys: 44.5 s, total: 6min 46s\n",
      "Wall time: 20min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Oversampling with Logistic regression & TDIDF\n",
    "lemmatized_lr_ros_tfidf_test_report, lemmatized_lr_ros_tfidf_val_report = lr_classification(df['lemmatized_lyrics'], df['tag'], oversample=True, vectorizer_method='tfidf', n_top_features=10)\n",
    "print(lemmatized_lr_ros_tfidf_test_report, lemmatized_lr_ros_tfidf_val_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
