{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -requiremnts_bert.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {'input_ids': torch.as_tensor(self.encodings.iloc[idx])}\n",
    "        item['labels'] = torch.as_tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "class LyricsClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=5): #@RIES TRY \"bert-large-uncased\" with the A100 (BUT  Tokenization needs also to be adjusted)\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(self.hparams.model_name,\n",
    "                                                                  num_labels=self.hparams.num_labels)\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\",compute_on_step=False, num_classes=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        return self.bert(input_ids, labels=labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(batch['input_ids'], batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(batch['input_ids'], batch['labels'])\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct = (predicted == batch['labels']).sum().item()\n",
    "        accuracy = correct / len(batch['labels'])\n",
    "        self.log('val_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return accuracy\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "def load_data():\n",
    "    #load pickle files (data and labels)\n",
    "\n",
    "    with open(\"/content/drive/MyDrive/NLP/new_small_try/tokenized_lyrics_small.pickle\", 'rb') as f:\n",
    "        encodings = pickle.load(f)\n",
    "    with open(\"/content/drive/MyDrive/NLP/new_small_try/labels_small.pickle\", 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "\n",
    "    #split into training and validation + test set\n",
    "    train_encodings, train_labels, val_test_encodings, val_test_labels = train_test_split(encodings, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    #split validation set into validation and test set\n",
    "    val_encodings, val_labels, test_encodings, test_labels = train_test_split(val_test_encodings, val_test_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "    return train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels\n",
    "\n",
    "\n",
    "#prepare tokenizer and data collator\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#prepare datasets\n",
    "train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels = load_data()\n",
    "\n",
    "train_dataset = LyricsDataset(train_encodings, train_labels)\n",
    "val_dataset = LyricsDataset(val_encodings, val_labels)\n",
    "test_dataset = LyricsDataset(test_encodings, test_labels)\n",
    "\n",
    "model = LyricsClassifier()\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "#Prepare trainer\n",
    "trainer = pl.Trainer(precision=16, limit_train_batches=0.5,max_epochs=3)\n",
    "\n",
    "# Training\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model from checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in model from checkpoint instead of training\n",
    "#model = LyricsClassifier.load_from_checkpoint(checkpoint_path=\"/content/lightning_logs/version_3/checkpoints/epoch=0-step=1406.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=data_collator, num_workers=4)\n",
    "\n",
    "# Move model to device once\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "    batch_input_ids = batch['input_ids'].to(device)  \n",
    "    batch_labels = batch['labels'].to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():  \n",
    "        outputs = model(batch_input_ids, batch_labels)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs.logits, 1)\n",
    "    predicted_labels.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
