{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING WORD2VEC MODEL ON CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/work/NLP_Project/word2vec_tokenized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>lyrics_word_count</th>\n",
       "      <th>tokenized_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>killa cam killa cam cam killa cam killa cam k...</td>\n",
       "      <td>1</td>\n",
       "      <td>762</td>\n",
       "      <td>[killa, cam, killa, cam, cam, killa, cam, kill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>yeah hah yeah rocafella we invite you to so...</td>\n",
       "      <td>3</td>\n",
       "      <td>548</td>\n",
       "      <td>[yeah, hah, yeah, rocafella, we, invite, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>maybe cause im eatin and these bastards fiend ...</td>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "      <td>[maybe, cause, im, eatin, and, these, bastards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>ugh killa baby kanye this that 1970s heron ...</td>\n",
       "      <td>5</td>\n",
       "      <td>760</td>\n",
       "      <td>[ugh, killa, baby, kanye, this, that, 1970s, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>so they ask me young boy what you gon do the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>432</td>\n",
       "      <td>[so, they, ask, me, young, boy, what, you, gon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913399</th>\n",
       "      <td>Everything Is Alright Now</td>\n",
       "      <td>pop</td>\n",
       "      <td>Chuck Bernard</td>\n",
       "      <td>2013</td>\n",
       "      <td>everything is alright now oh yes baby everythi...</td>\n",
       "      <td>7882838</td>\n",
       "      <td>63</td>\n",
       "      <td>[everything, is, alright, now, oh, yes, baby, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913401</th>\n",
       "      <td>White Lies</td>\n",
       "      <td>pop</td>\n",
       "      <td>ElementD</td>\n",
       "      <td>2019</td>\n",
       "      <td>half truth and half you didnt we say were thr...</td>\n",
       "      <td>7882840</td>\n",
       "      <td>171</td>\n",
       "      <td>[half, truth, and, half, you, didnt, we, say, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913403</th>\n",
       "      <td>Ocean</td>\n",
       "      <td>pop</td>\n",
       "      <td>Effemar</td>\n",
       "      <td>2022</td>\n",
       "      <td>dance for me now keeping yourself moving your...</td>\n",
       "      <td>7882842</td>\n",
       "      <td>166</td>\n",
       "      <td>[dance, for, me, now, keeping, yourself, movin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913406</th>\n",
       "      <td>Raise Our Hands</td>\n",
       "      <td>pop</td>\n",
       "      <td>Culture Code, Pag &amp; Mylo</td>\n",
       "      <td>2016</td>\n",
       "      <td>here our purpose feels alive we are more than...</td>\n",
       "      <td>7882845</td>\n",
       "      <td>184</td>\n",
       "      <td>[here, our, purpose, feels, alive, we, are, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913409</th>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>2022</td>\n",
       "      <td>you need a new number one that aint burned in...</td>\n",
       "      <td>7882848</td>\n",
       "      <td>293</td>\n",
       "      <td>[you, need, a, new, number, one, that, aint, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3315185 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title      tag                    artist  year   \n",
       "0                        Killa Cam      rap                   Cam'ron  2004  \\\n",
       "1                       Can I Live      rap                     JAY-Z  1996   \n",
       "2                Forgive Me Father      rap                  Fabolous  2003   \n",
       "3                     Down and Out      rap                   Cam'ron  2004   \n",
       "4                           Fly In      rap                 Lil Wayne  2005   \n",
       "...                            ...      ...                       ...   ...   \n",
       "5913399  Everything Is Alright Now      pop             Chuck Bernard  2013   \n",
       "5913401                 White Lies      pop                  ElementD  2019   \n",
       "5913403                      Ocean      pop                   Effemar  2022   \n",
       "5913406            Raise Our Hands      pop  Culture Code, Pag & Mylo  2016   \n",
       "5913409                 New Number  country         Alana Springsteen  2022   \n",
       "\n",
       "                                                    lyrics       id   \n",
       "0         killa cam killa cam cam killa cam killa cam k...        1  \\\n",
       "1           yeah hah yeah rocafella we invite you to so...        3   \n",
       "2        maybe cause im eatin and these bastards fiend ...        4   \n",
       "3           ugh killa baby kanye this that 1970s heron ...        5   \n",
       "4         so they ask me young boy what you gon do the ...        6   \n",
       "...                                                    ...      ...   \n",
       "5913399  everything is alright now oh yes baby everythi...  7882838   \n",
       "5913401   half truth and half you didnt we say were thr...  7882840   \n",
       "5913403   dance for me now keeping yourself moving your...  7882842   \n",
       "5913406   here our purpose feels alive we are more than...  7882845   \n",
       "5913409   you need a new number one that aint burned in...  7882848   \n",
       "\n",
       "         lyrics_word_count                                   tokenized_lyrics  \n",
       "0                      762  [killa, cam, killa, cam, cam, killa, cam, kill...  \n",
       "1                      548  [yeah, hah, yeah, rocafella, we, invite, you, ...  \n",
       "2                      574  [maybe, cause, im, eatin, and, these, bastards...  \n",
       "3                      760  [ugh, killa, baby, kanye, this, that, 1970s, h...  \n",
       "4                      432  [so, they, ask, me, young, boy, what, you, gon...  \n",
       "...                    ...                                                ...  \n",
       "5913399                 63  [everything, is, alright, now, oh, yes, baby, ...  \n",
       "5913401                171  [half, truth, and, half, you, didnt, we, say, ...  \n",
       "5913403                166  [dance, for, me, now, keeping, yourself, movin...  \n",
       "5913406                184  [here, our, purpose, feels, alive, we, are, mo...  \n",
       "5913409                293  [you, need, a, new, number, one, that, aint, b...  \n",
       "\n",
       "[3315185 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(df['tokenized_lyrics'].to_list(), min_count=5, workers=31, window=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"original_w2v.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec.load(\"/work/NLP_Project/GenreFromLyricsShared/Word2VecModels/original_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - TOKENIZE WORDS TO INDICES\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# tokenizer needs a list of texts - df column is a Series - pass list of lists \n",
    "tokenizer.fit_on_texts(df['tokenized_lyrics'].tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2 - TRANSFORM TEXT TO SEQUENCES - TOKENIZER CONVERTS LYRICS INTO SEQUENCE OF INTEGERS\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['tokenized_lyrics'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 - CALCULATE SEQUENCE LENGTH - 75TH PERCENTILE AS TOO LARGE TO TAKE MORE - 374 WORDS\n",
    "\n",
    "#calculate 75th percentile seq length\n",
    "lengths = [len(sequence) for sequence in sequences]\n",
    "max_sequence_length = int(np.percentile(lengths, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4 - TRUNCATE OR PAD LYRICS TO THE 374TH INTEGER\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3315185"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 - CREATING AN 'EMBEDDING MATRIX' - NUMWORDS * EMBEDDING DIMENSION - EACH ROW REPRESENTS A WORDS EMBEDDING VECTOR.\n",
    "# ITERATE OVER EACH WORD IN THE TOKENIZER VOCAB (ALL WORDS FROM THE TOKENIZED LYRICS COLUMN) - IF EXISTS, INCLUDE IN EMBEDDING MATRIX\n",
    "\n",
    "#columns in embedding matrix - same size as word2vec vector\n",
    "embedding_dim = 100  \n",
    "\n",
    "#zero matrix \n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "#iterate through through tokenizer vocab - if word is in the word2vec model vocab, find vector and add it to matrix at same index\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv.key_to_index:\n",
    "        embedding_vector = w2v_model.wv[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3032678, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 08:13:14.421441: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding, LSTM, Dense\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m---> 14\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(input_dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m                     output_dim\u001b[39m=\u001b[39membedding_dim,\n\u001b[1;32m     16\u001b[0m                     weights\u001b[39m=\u001b[39m[embedding_matrix],\n\u001b[1;32m     17\u001b[0m                     input_length\u001b[39m=\u001b[39mmax_sequence_length,\n\u001b[1;32m     18\u001b[0m                     trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m     19\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m64\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m))\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m'\u001b[39m])), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# STEP 6 - BUILD MODEL, WHICH INCLUDES EMBEDDING LAYER. WE HAVE PRE-TRAINED OUR EMBEDDINGS WITH THE WORD2VEC EMBEDDINGS SO IT DOES NOT TRAIN / LEARN FROM THE DATA IN A STANDARD WAY.\n",
    "# WE NEED THIS EMBEDDING LAYER AS CAN'T FEED RAW WORDS INTO NN - TOO SPARSE. \n",
    "# INPUT DIM = VOCAB SIZE\n",
    "# OUTPUT DIM = VECTOR SPACE SIZE IN WHICH WORDS ARE EMBEDDED - WE CHOSE 100 IN WORD2VEC\n",
    "# WEIGHTS = EMBEDDING MATRIX CHOSEN. THE ITH ROW IS THE PRE-TRAINED VECTOR THE WORD OF INDEX I\n",
    "# INPUT LENGTH = THE MAX LENGTH WE FEED IN - WE TRUNCATED/PADDED TO 374\n",
    "# DROPOUT - HELPS TO PREVENT OVERTFITTING BY ADDING NOISE TO OUTPUTS - GENERALIZES BETTER\n",
    "# RECURRENT DROPOUT - APPLIED TO RECURRENT INPUTS - RANDOMLY SETS FRACTION OF INPUT UNITS TO 0 AT EACH UPDATE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_sequence_length,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(set(df['tag'])), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 08:33:01.245790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-24 08:33:01.247473: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-24 08:33:01.248832: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 374, 100)          303267800 \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303,310,365\n",
      "Trainable params: 42,565\n",
      "Non-trainable params: 303,267,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# STEP 6 - BUILD MODEL, WHICH INCLUDES EMBEDDING LAYER. WE HAVE PRE-TRAINED OUR EMBEDDINGS WITH THE WORD2VEC EMBEDDINGS SO IT DOES NOT TRAIN / LEARN FROM THE DATA IN A STANDARD WAY.\n",
    "# WE NEED THIS EMBEDDING LAYER AS CAN'T FEED RAW WORDS INTO NN - TOO SPARSE. \n",
    "# INPUT DIM = VOCAB SIZE\n",
    "# OUTPUT DIM = VECTOR SPACE SIZE IN WHICH WORDS ARE EMBEDDED - WE CHOSE 100 IN WORD2VEC\n",
    "# WEIGHTS = EMBEDDING MATRIX CHOSEN. THE ITH ROW IS THE PRE-TRAINED VECTOR THE WORD OF INDEX I\n",
    "# INPUT LENGTH = THE MAX LENGTH WE FEED IN - WE TRUNCATED/PADDED TO 374\n",
    "# DROPOUT - HELPS TO PREVENT OVERTFITTING BY ADDING NOISE TO OUTPUTS - GENERALIZES BETTER\n",
    "# RECURRENT DROPOUT - APPLIED TO RECURRENT INPUTS - RANDOMLY SETS FRACTION OF INPUT UNITS TO 0 AT EACH UPDATE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_sequence_length,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2))\n",
    "model.add(Dense(len(set(df['tag'])), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 08:33:12.503292: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-24 08:33:12.505467: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-24 08:33:12.506625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-24 08:33:13.155535: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-24 08:33:13.157480: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-24 08:33:13.158693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36260/36260 [==============================] - ETA: 0s - loss: 0.8353 - accuracy: 0.6703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:18:05.676033: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-24 10:18:05.677912: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-24 10:18:05.679098: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36260/36260 [==============================] - 8200s 226ms/step - loss: 0.8353 - accuracy: 0.6703 - val_loss: 0.8046 - val_accuracy: 0.6790\n",
      "Epoch 2/5\n",
      "36260/36260 [==============================] - 8029s 221ms/step - loss: 0.7985 - accuracy: 0.6818 - val_loss: 0.7910 - val_accuracy: 0.6847\n",
      "Epoch 3/5\n",
      "36260/36260 [==============================] - 7974s 220ms/step - loss: 0.7923 - accuracy: 0.6836 - val_loss: 0.7846 - val_accuracy: 0.6879\n",
      "Epoch 4/5\n",
      "36260/36260 [==============================] - 8089s 223ms/step - loss: 0.7866 - accuracy: 0.6857 - val_loss: 0.7840 - val_accuracy: 0.6865\n",
      "Epoch 5/5\n",
      "36260/36260 [==============================] - 8064s 222ms/step - loss: 0.7838 - accuracy: 0.6862 - val_loss: 0.7779 - val_accuracy: 0.6893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#genres converted to integer labels and then into one-hot format for categorical cross entropy \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df['tag'])\n",
    "categorical_labels = to_categorical(integer_encoded)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(sequences, categorical_labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:09:03.940186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-24 20:09:03.942098: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-24 20:09:03.943215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31080/31080 [==============================] - 1648s 53ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.56      0.03      0.05     25477\n",
      "         pop       0.62      0.87      0.73    430965\n",
      "         rap       0.85      0.89      0.87    298959\n",
      "          rb       0.38      0.08      0.13     47343\n",
      "        rock       0.59      0.22      0.32    191812\n",
      "\n",
      "    accuracy                           0.69    994556\n",
      "   macro avg       0.60      0.42      0.42    994556\n",
      "weighted avg       0.67      0.69      0.64    994556\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "\n",
    "# Predict class probabilities on the test set\n",
    "y_prob = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = argmax(y_prob, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test to class labels\n",
    "y_true = argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
