{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classification(train, label, vectorizer='bow', n_top_features=10, oversample=False):\n",
    "\n",
    "    train = train.apply(' '.join)\n",
    "\n",
    "    # split into train and test sets, with stratifying\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(train, label, test_size=0.3, random_state=42, stratify=label)\n",
    "\n",
    "    # Split the temporary test set into 50% test and 50% validation (15% of the total data each)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    if vectorizer == 'bow':\n",
    "        vec = CountVectorizer()\n",
    "    elif vectorizer == 'tfidf':\n",
    "        vec = TfidfVectorizer()\n",
    "\n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "\n",
    "    # Initialize the MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # Perform random oversampling if enabled\n",
    "    if oversample:\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "        X_train_vec, y_train = oversampler.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    nb.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Print the most informative features\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    for i, class_label in enumerate(nb.classes_):\n",
    "        print(f\"\\nClass: {class_label}\")\n",
    "        top_features_idx = nb.feature_log_prob_[i].argsort()[-n_top_features:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_features_idx]\n",
    "        print(\", \".join(top_features))\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = nb.predict(X_test_vec)\n",
    "    y_val_pred = nb.predict(X_val_vec)\n",
    "\n",
    "    # Generate classification report\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "    return test_report, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_classification(train, label, vectorizer_method='tfidf', oversample=False, n_top_features=10):\n",
    "\n",
    "    train = train.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # split into train and test sets, with stratifying\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(train, label, test_size=0.3, random_state=42, stratify=label)\n",
    "\n",
    "    # Split the temporary test set into 50% test and 50% validation (15% of the total data each)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    # Initialize vectorizer (TDIDF OR bag of words)\n",
    "    if vectorizer_method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif vectorizer_method == 'bow':\n",
    "        vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Vectorize data\n",
    "    X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "    X_val_vectors = vectorizer.transform(X_val)\n",
    "    X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "    # Perform oversampling\n",
    "    if oversample == True:\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "        X_train_vectors, y_train = oversampler.fit_resample(X_train_vectors, y_train)\n",
    "\n",
    "    # Initialize the Multinomial LR\n",
    "    lr_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "    # Fit the model to the training data \n",
    "    lr_clf.fit(X_train_vectors, y_train)\n",
    "  \n",
    "    # Get the feature names from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "  \n",
    "    # Print the most informative features\n",
    "    for i, label in enumerate(lr_clf.classes_):\n",
    "        print(f\"\\nClass: {label}\")\n",
    "        top_features_idx = lr_clf.coef_[i].argsort()[-n_top_features:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_features_idx]\n",
    "        print(\", \".join(top_features))\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = lr_clf.predict(X_test_vectors)\n",
    "    y_val_pred = lr_clf.predict(X_val_vectors)\n",
    "\n",
    "    # Generate classification report\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "    return test_report, val_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./pkl_files/tokenized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>lyrics_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>[killa, cam, killa, cam, cam, killa, cam, kill...</td>\n",
       "      <td>1</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>[yeah, hah, yeah, rocafella, invite, somethin,...</td>\n",
       "      <td>3</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>[maybe, cause, im, eatin, bastards, fiend, gru...</td>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>[ugh, killa, baby, kanye, 1970s, heron, flow, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>[ask, young, boy, gon, second, time, around, g...</td>\n",
       "      <td>6</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   \n",
       "0          Killa Cam  rap    Cam'ron  2004  \\\n",
       "1         Can I Live  rap      JAY-Z  1996   \n",
       "2  Forgive Me Father  rap   Fabolous  2003   \n",
       "3       Down and Out  rap    Cam'ron  2004   \n",
       "4             Fly In  rap  Lil Wayne  2005   \n",
       "\n",
       "                                              lyrics  id  lyrics_word_count  \n",
       "0  [killa, cam, killa, cam, cam, killa, cam, kill...   1                762  \n",
       "1  [yeah, hah, yeah, rocafella, invite, somethin,...   3                548  \n",
       "2  [maybe, cause, im, eatin, bastards, fiend, gru...   4                574  \n",
       "3  [ugh, killa, baby, kanye, 1970s, heron, flow, ...   5                760  \n",
       "4  [ask, young, boy, gon, second, time, around, g...   6                432  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "im, love, like, dont, know, got, na, oh, one, time\n",
      "\n",
      "Class: pop\n",
      "im, love, know, dont, like, na, oh, got, go, time\n",
      "\n",
      "Class: rap\n",
      "im, like, got, know, get, yeah, dont, shit, aint, na\n",
      "\n",
      "Class: rb\n",
      "love, know, im, yeah, dont, got, like, baby, na, oh\n",
      "\n",
      "Class: rock\n",
      "im, know, dont, like, time, love, na, never, oh, got\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.33      0.18      0.23     12677\n",
      "         pop       0.65      0.49      0.56    215359\n",
      "         rap       0.83      0.79      0.81    149486\n",
      "          rb       0.25      0.35      0.29     23802\n",
      "        rock       0.39      0.62      0.48     95954\n",
      "\n",
      "    accuracy                           0.59    497278\n",
      "   macro avg       0.49      0.49      0.47    497278\n",
      "weighted avg       0.63      0.59      0.60    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.32      0.18      0.23     12678\n",
      "         pop       0.65      0.49      0.56    215358\n",
      "         rap       0.83      0.79      0.81    149487\n",
      "          rb       0.24      0.34      0.29     23802\n",
      "        rock       0.40      0.62      0.48     95953\n",
      "\n",
      "    accuracy                           0.59    497278\n",
      "   macro avg       0.49      0.49      0.47    497278\n",
      "weighted avg       0.63      0.59      0.60    497278\n",
      "\n",
      "CPU times: user 6min 31s, sys: 20 s, total: 6min 51s\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes & Bag of Words\n",
    "tokenized_nb_bow_test_report, tokenized_nb_bow_val_report = nb_classification(df['lyrics'], df['tag'], vectorizer='bow')\n",
    "print(tokenized_nb_bow_test_report, tokenized_nb_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "love, im, dont, know, like, oh, got, na, one, youre\n",
      "\n",
      "Class: pop\n",
      "love, im, dont, know, oh, na, like, youre, time, go\n",
      "\n",
      "Class: rap\n",
      "im, like, got, yeah, get, dont, shit, bitch, know, nigga\n",
      "\n",
      "Class: rb\n",
      "love, baby, yeah, know, im, dont, oh, na, got, like\n",
      "\n",
      "Class: rock\n",
      "im, dont, know, time, love, youre, never, oh, like, see\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.00      0.00      0.00     12677\n",
      "         pop       0.58      0.91      0.71    215359\n",
      "         rap       0.80      0.83      0.82    149486\n",
      "          rb       0.16      0.00      0.00     23802\n",
      "        rock       0.70      0.05      0.09     95954\n",
      "\n",
      "    accuracy                           0.65    497278\n",
      "   macro avg       0.45      0.36      0.32    497278\n",
      "weighted avg       0.64      0.65      0.57    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.00      0.00      0.00     12678\n",
      "         pop       0.58      0.91      0.71    215358\n",
      "         rap       0.80      0.83      0.82    149487\n",
      "          rb       0.12      0.00      0.00     23802\n",
      "        rock       0.69      0.05      0.09     95953\n",
      "\n",
      "    accuracy                           0.65    497278\n",
      "   macro avg       0.44      0.36      0.32    497278\n",
      "weighted avg       0.63      0.65      0.57    497278\n",
      "\n",
      "CPU times: user 6min 37s, sys: 16.6 s, total: 6min 53s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes & TFIDF\n",
    "tokenized_nb_tfidf_test_report, tokenized_nb_tfidf_val_report = nb_classification(df['lyrics'], df['tag'], vectorizer='tfidf')\n",
    "print(tokenized_nb_tfidf_test_report, tokenized_nb_tfidf_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "im, love, like, dont, know, got, na, oh, one, time\n",
      "\n",
      "Class: pop\n",
      "im, love, know, dont, like, na, oh, got, go, time\n",
      "\n",
      "Class: rap\n",
      "im, like, got, know, get, yeah, dont, shit, aint, na\n",
      "\n",
      "Class: rb\n",
      "love, know, im, yeah, dont, got, like, baby, na, oh\n",
      "\n",
      "Class: rock\n",
      "im, know, dont, like, time, love, never, oh, na, got\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.14      0.67      0.23     12677\n",
      "         pop       0.70      0.18      0.29    215359\n",
      "         rap       0.86      0.75      0.80    149486\n",
      "          rb       0.17      0.59      0.26     23802\n",
      "        rock       0.38      0.65      0.48     95954\n",
      "\n",
      "    accuracy                           0.48    497278\n",
      "   macro avg       0.45      0.57      0.41    497278\n",
      "weighted avg       0.65      0.48      0.48    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.14      0.67      0.23     12678\n",
      "         pop       0.70      0.18      0.29    215358\n",
      "         rap       0.86      0.75      0.80    149487\n",
      "          rb       0.17      0.59      0.26     23802\n",
      "        rock       0.38      0.65      0.48     95953\n",
      "\n",
      "    accuracy                           0.48    497278\n",
      "   macro avg       0.45      0.57      0.41    497278\n",
      "weighted avg       0.65      0.48      0.48    497278\n",
      "\n",
      "CPU times: user 6min 55s, sys: 15.5 s, total: 7min 10s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Oversampling with Naive Bayes & Bag of Words\n",
    "tokenized_nb_ros_bow_test_report, tokenized_nb_ros_bow_val_report = nb_classification(df['lyrics'], df['tag'], vectorizer='bow', oversample=True)\n",
    "print(tokenized_nb_ros_bow_test_report, tokenized_nb_ros_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "lonesome, truck, porch, whiskey, tennessee, guitar, texas, memory, cowboy, folks\n",
      "\n",
      "Class: pop\n",
      "rap, fk, fcking, fck, chuckle, pre, niggas, spoken, fking, endless\n",
      "\n",
      "Class: rap\n",
      "rapping, snippet, lyrics, rapper, rappers, rap, intro, raps, fam, rhymes\n",
      "\n",
      "Class: rb\n",
      "pre, tryna, outro, trynna, niggas, 2x, hook, imma, nigga, shawty\n",
      "\n",
      "Class: rock\n",
      "thе, endless, fz, disease, punk, guitar, teeth, failure, decay, tongues\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.47      0.12      0.19     12677\n",
      "         pop       0.61      0.86      0.71    215359\n",
      "         rap       0.87      0.82      0.85    149486\n",
      "          rb       0.40      0.10      0.16     23802\n",
      "        rock       0.57      0.25      0.35     95954\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.58      0.43      0.45    497278\n",
      "weighted avg       0.67      0.68      0.64    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.46      0.12      0.19     12678\n",
      "         pop       0.61      0.86      0.71    215358\n",
      "         rap       0.87      0.82      0.85    149487\n",
      "          rb       0.40      0.10      0.16     23802\n",
      "        rock       0.57      0.25      0.35     95953\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.58      0.43      0.45    497278\n",
      "weighted avg       0.67      0.68      0.64    497278\n",
      "\n",
      "CPU times: user 6min 27s, sys: 27.1 s, total: 6min 54s\n",
      "Wall time: 16min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic regression & Bag of Words\n",
    "tokenized_lr_bow_test_report, tokenized_lr_bow_val_report = lr_classification(df['lyrics'], df['tag'], vectorizer_method='bow', n_top_features=10)\n",
    "print(tokenized_lr_bow_test_report, tokenized_lr_bow_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "country, whiskey, old, beer, truck, town, id, well, lonesome, cowboy\n",
      "\n",
      "Class: pop\n",
      "repeat, pre, fcking, cos, fck, spoken, dey, yo, fk, well\n",
      "\n",
      "Class: rap\n",
      "rap, hook, li, bro, rhymes, gang, rappers, bitch, tryna, bars\n",
      "\n",
      "Class: rb\n",
      "tryna, hook, ima, nigga, imma, pre, vibe, bout, shit, x2\n",
      "\n",
      "Class: rock\n",
      "thе, fucking, well, band, sick, death, teeth, dead, void, guitar\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.52      0.11      0.18     12677\n",
      "         pop       0.62      0.85      0.72    215359\n",
      "         rap       0.86      0.85      0.85    149486\n",
      "          rb       0.47      0.08      0.14     23802\n",
      "        rock       0.57      0.29      0.38     95954\n",
      "\n",
      "    accuracy                           0.69    497278\n",
      "   macro avg       0.61      0.44      0.46    497278\n",
      "weighted avg       0.67      0.69      0.65    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.51      0.11      0.18     12678\n",
      "         pop       0.62      0.85      0.72    215358\n",
      "         rap       0.86      0.85      0.85    149487\n",
      "          rb       0.46      0.08      0.14     23802\n",
      "        rock       0.56      0.29      0.38     95953\n",
      "\n",
      "    accuracy                           0.68    497278\n",
      "   macro avg       0.60      0.44      0.45    497278\n",
      "weighted avg       0.67      0.68      0.65    497278\n",
      "\n",
      "CPU times: user 6min 37s, sys: 30.7 s, total: 7min 7s\n",
      "Wall time: 16min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic regression & TDIDF\n",
    "tokenized_lr_tfidf_test_report, tokenized_lr_tfidf_val_report = lr_classification(df['lyrics'], df['tag'], vectorizer_method='tfidf', n_top_features=10)\n",
    "print(tokenized_lr_tfidf_test_report, tokenized_lr_tfidf_val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect RAM from garbage to prevent kernel from dying\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: country\n",
      "truck, whiskey, country, beer, cowboy, bar, ol, tennessee, old, boots\n",
      "\n",
      "Class: pop\n",
      "cos, repeat, spoken, yo, pre, bitch, fcking, endless, club, noone\n",
      "\n",
      "Class: rap\n",
      "rap, tryna, hook, nigga, niggas, rappers, rhymes, li, shit, yo\n",
      "\n",
      "Class: rb\n",
      "tryna, hook, pre, niggas, imma, nigga, ima, vibe, yo, shit\n",
      "\n",
      "Class: rock\n",
      "disease, fucking, band, void, teeth, punk, well, drag, destroy, sick\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.15      0.69      0.24     12677\n",
      "         pop       0.69      0.32      0.44    215359\n",
      "         rap       0.89      0.79      0.83    149486\n",
      "          rb       0.19      0.59      0.28     23802\n",
      "        rock       0.44      0.59      0.50     95954\n",
      "\n",
      "    accuracy                           0.54    497278\n",
      "   macro avg       0.47      0.60      0.46    497278\n",
      "weighted avg       0.67      0.54      0.56    497278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.15      0.70      0.25     12678\n",
      "         pop       0.69      0.32      0.44    215358\n",
      "         rap       0.89      0.79      0.83    149487\n",
      "          rb       0.19      0.59      0.28     23802\n",
      "        rock       0.44      0.59      0.50     95953\n",
      "\n",
      "    accuracy                           0.54    497278\n",
      "   macro avg       0.47      0.60      0.46    497278\n",
      "weighted avg       0.67      0.54      0.56    497278\n",
      "\n",
      "CPU times: user 6min 51s, sys: 34.2 s, total: 7min 25s\n",
      "Wall time: 25min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Oversampling with Logistic regression & TDIDF\n",
    "tokenized_lr_ros_tfidf_test_report, tokenized_lr_ros_tfidf_val_report = lr_classification(df['lyrics'], df['tag'], oversample=True, vectorizer_method='tfidf', n_top_features=10)\n",
    "print(tokenized_lr_ros_tfidf_test_report, tokenized_lr_ros_tfidf_val_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
