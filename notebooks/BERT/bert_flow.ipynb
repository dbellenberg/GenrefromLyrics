{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this bert flow is based on and requires \"df_cleaned_engl.pkl\"\n",
    "\n",
    "(for memory effieciency run prep & tokenization seperate from training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -requiremnts_bert.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep & tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_lyrics(lyrics):\n",
    "    # Remove strings enclosed in brackets []\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    \n",
    "    # Remove substrings starting with a backslash \\\n",
    "    lyrics = re.sub(r'\\\\[^\\s]*', '', lyrics)\n",
    "\n",
    "    # Remove newline characters \\n\n",
    "    lyrics = re.sub(r'\\n', ' ', lyrics)\n",
    "    \n",
    "    # Remove single quotes '\n",
    "    lyrics = re.sub(r\"'\", '', lyrics)\n",
    "    \n",
    "    # Remove leading and trailing whitespaces\n",
    "    lyrics = lyrics.strip()\n",
    "\n",
    "    # Strip the string and ensure only one space between words\n",
    "    lyrics = re.sub(r'\\s+', ' ', lyrics.strip())\n",
    "\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subset(df, n=None, p=None):\n",
    "    if n is not None: # If sample size is provided\n",
    "        df_sub = df.groupby(\"tag\").apply(lambda x: x.sample(n=n, random_state=1)).reset_index(drop=True)\n",
    "    elif p is not None: # If sample percentage is provided\n",
    "        df_sub = df.groupby(\"tag\").apply(lambda x: x.sample(n=int(np.ceil(x.shape[0]*p)), random_state=1)).reset_index(drop=True)\n",
    "    \n",
    "    #print create subset with n samples per tag\n",
    "    print(f\"Subset created with {df_sub.shape[0]} samples\")\n",
    "    \n",
    "    #drop uncessary columns\n",
    "    df_sub.drop(columns=[\"title\",'artist', 'year',\"id\",\"language\",\"word_count\"], inplace=True)\n",
    "\n",
    "    # apply strip_lyrics (re)\n",
    "    print(\"Applying strip_lyrics...\")\n",
    "    df_sub['lyrics'] = df_sub['lyrics'].progress_apply(lambda x: strip_lyrics(x))\n",
    "\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bert(dataframe):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    def tokenize_song(song):\n",
    "        tokenized = tokenizer.encode(song, max_length=512, truncation=True, padding='max_length')\n",
    "        return tokenized\n",
    "\n",
    "    # Pass both song and index using lambda function\n",
    "    print(\"Tokenizing...\")\n",
    "    tokenized = dataframe['lyrics'].reset_index().progress_apply(lambda x: tokenize_song((x['lyrics'])), axis=1)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas read pickle\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_pickle('/work/cleaned_df/df_cleaned_engl.pkl')\n",
    "print(\"Loaded in df with shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add word count\n",
    "print(\"Adding word count...\")\n",
    "df['word_count'] = df['lyrics'].progress_apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Filtering songs based on word count...\")\n",
    "df = df[(df['word_count'] < 5000) & (df['word_count'] > 25)]\n",
    "print(\"Reduced size to: \", df.shape)\n",
    "\n",
    "print(\"Filtering songs based on year...\")\n",
    "df = df[(df['year'] >= 1960) & (df['year'] <= 2023)]\n",
    "print(\"Reduced size to: \", df.shape)\n",
    "\n",
    "####### SET PARAMS HERE #######\n",
    "#generate traing and test set (validation set is split from trainin set later)\n",
    "print(\"Generating training subset...\")\n",
    "df_trainings_subsample = generate_subset(df, n=20000)\n",
    "print(\"Generating validation subset...\")\n",
    "df_validation_subsample = generate_subset(df, p=0.05) #here it is important to use percentage to keep the distribution of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokemize lyrics of training and validation set\n",
    "trainings_subsample_tokenized = tokenize_with_bert(df_trainings_subsample)\n",
    "validation_subsample_tokenized = tokenize_with_bert(df_validation_subsample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {'input_ids': torch.as_tensor(self.encodings.iloc[idx])}\n",
    "        item['labels'] = torch.as_tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "class LyricsClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(self.hparams.model_name,\n",
    "                                                                  num_labels=self.hparams.num_labels)\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\",compute_on_step=False, num_classes=num_labels)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        return self.bert(input_ids, labels=labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(batch['input_ids'], batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(batch['input_ids'], batch['labels'])\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct = (predicted == batch['labels']).sum().item()\n",
    "        accuracy = correct / len(batch['labels'])\n",
    "        self.log('val_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return accuracy\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    encodings = trainings_subsample_tokenized\n",
    "\n",
    "    #create labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_trainings_subsample['tag'] = label_encoder.fit_transform(df_trainings_subsample['tag'])\n",
    "    \n",
    "    labels = df_trainings_subsample['tag']\n",
    "\n",
    "    return encodings, labels\n",
    "\n",
    "def main():\n",
    "    encodings, labels = load_data()\n",
    "\n",
    "    #prepare tokenizer and data collator\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    #prepare datasets\n",
    "    train_encodings, val_encodings, train_labels, val_labels = train_test_split(encodings, labels, test_size=0.1, random_state=42)\n",
    "    train_dataset = LyricsDataset(train_encodings, train_labels)\n",
    "    val_dataset = LyricsDataset(val_encodings, val_labels)\n",
    "\n",
    "    model = LyricsClassifier()\n",
    "\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "    #Prepare trainer\n",
    "    trainer = pl.Trainer(precision=16, limit_train_batches=0.5,max_epochs=3)\n",
    "\n",
    "    # Training\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    return model, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run actual training\n",
    "model, data_collator = main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in model from checkpoint instead of training\n",
    "#model = LyricsClassifier.load_from_checkpoint(checkpoint_path=\"/content/lightning_logs/version_3/checkpoints/epoch=0-step=1406.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##this is not tested yet in the flow but was adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = validation_subsample_tokenized\n",
    "label_encoder = LabelEncoder()\n",
    "df_validation_subsample['tag'] = label_encoder.fit_transform(df_validation_subsample['tag'])\n",
    "test_labels = df_validation_subsample['tag']\n",
    "test_dataset = LyricsDataset(test_encodings, test_labels)\n",
    "\n",
    "\n",
    "#ADJUST IF POSSIBLE ON LARGE GPU!!!! LOWER INFERENCE TIME WHEN BATCH SIZE IS HIGHER BUT ALSO MORE MEMORY CONSUMPTION\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=data_collator, num_workers=4)\n",
    "\n",
    "# Move model to device once\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "    batch_input_ids = batch['input_ids'].to(device)  \n",
    "    batch_labels = batch['labels'].to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():  \n",
    "        outputs = model(batch_input_ids, batch_labels)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    _, preds = torch.max(outputs.logits, 1)\n",
    "    predicted_labels.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
