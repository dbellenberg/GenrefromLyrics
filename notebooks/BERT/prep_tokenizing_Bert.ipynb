{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_lyrics(lyrics):\n",
    "    # Remove strings enclosed in brackets []\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    \n",
    "    # Remove substrings starting with a backslash \\\n",
    "    lyrics = re.sub(r'\\\\[^\\s]*', '', lyrics)\n",
    "\n",
    "    # Remove newline characters \\n\n",
    "    lyrics = re.sub(r'\\n', ' ', lyrics)\n",
    "    \n",
    "    # Remove single quotes '\n",
    "    lyrics = re.sub(r\"'\", '', lyrics)\n",
    "    \n",
    "    # Remove leading and trailing whitespaces\n",
    "    lyrics = lyrics.strip()\n",
    "\n",
    "    # Strip the string and ensure only one space between words\n",
    "    lyrics = re.sub(r'\\s+', ' ', lyrics.strip())\n",
    "\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/work/cleaned_df/df_cleaned_engl.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#add word count\n",
    "df['word_count'] = df['lyrics'].progress_apply(lambda x: len(x.split()))\n",
    "\n",
    "df = df[(df['word_count'] < 5000) & (df['word_count'] > 25)]\n",
    "df = df[(df['year'] >= 1960) & (df['year'] <= 2023)]\n",
    "\n",
    "\n",
    "#generate subset\n",
    "sample_size = 20000\n",
    "\n",
    "df_sub = df.groupby('tag').progress_apply(lambda x: x.sample(n=sample_size, random_state=1)).reset_index(drop=True)\n",
    "\n",
    "#drop columns of subset\n",
    "df_sub.drop(columns=[\"title\",'artist', 'year',\"id\",\"language\",\"word_count\"], inplace=True)\n",
    "\n",
    "# apply strip_lyrics (re)\n",
    "df_sub['lyrics'] = df_sub['lyrics'].progress_apply(lambda x: strip_lyrics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if shape of df_sub = (sample_size, 2)    \n",
    "df_sub.shape == (sample_size*5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "#df_sub.to_csv(\"work/newsubset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bert(dataframe, column_name):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    def tokenize_song(song):\n",
    "        tokenized = tokenizer.encode(song, max_length=512, truncation=True, padding='max_length')\n",
    "        return tokenized\n",
    "\n",
    "    # Pass both song and index using lambda function\n",
    "    tokenized = dataframe[column_name].reset_index().progress_apply(lambda x: tokenize_song((x[column_name])), axis=1)\n",
    "    return tokenized\n",
    "\n",
    "# Example usage\n",
    "tokenized_lyrics = tokenize_with_bert(df_sub, 'lyrics')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving tokenized data (& labels) to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tokenized lyrics and labels\n",
    "with open('tokenized_lyrics.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenized_lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#label encode \"tag\" column\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_sub_sample['tag'] = le.fit_transform(df_sub_sample['tag'])\n",
    "\n",
    "with open('labels_le.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_sub_sample['tag'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
